# PersonaLive 代码架构与调用链分析

## 一、项目目录结构

```
PersonaLive/
├── inference_offline.py      # 离线批量视频生成入口
├── inference_online.py       # 在线流式推理入口 (FastAPI)
├── src/                      # 核心源代码
│   ├── pipelines/            # Pipeline 实现
│   │   └── pipeline_pose2vid.py   # 姿态到视频的 Pipeline
│   ├── models/               # 模型定义
│   │   ├── unet_2d_condition.py  # 参考 UNet (2D)
│   │   ├── unet_3d.py            # 去噪 UNet (3D)
│   │   ├── motion_encoder/        # 运动编码器
│   │   │   └── motion_encoder.py
│   │   └── pose_guider.py         # 姿态引导器
│   ├── liveportrait/         # LivePortrait 组件
│   │   └── motion_extractor.py    # 3D 关键点提取器
│   ├── scheduler/            # 调度器
│   │   └── ddim.py
│   ├── utils/                # 工具函数
│   └── wrapper.py            # PersonaLive 包装类
├── webcam/                   # 在线推理 Web 界面
│   ├── vid2vid.py            # 视频到视频 Pipeline
│   ├── vid2vid_trt.py        # TensorRT 加速 Pipeline
│   └── config.py             # 配置
├── configs/                  # 配置文件
│   ├── inference/            # 模型推理配置
│   └── prompts/              # Prompt 和模型路径配置
├── demo/                     # 示例资源
├── pretrained_weights/       # 预训练权重
└── tools/                    # 工具脚本
```

---

## 二、入口文件分析

### 2.1 离线推理入口 (`inference_offline.py`)

**调用链:**

```
inference_offline.py
    │
    ├──► 加载配置 (configs/prompts/personalive_offline.yaml)
    │
    ├──► 初始化模型
    │   ├── VAE (AutoencoderKL)
    │   ├── 图像编码器 (CLIP-ViT-H/14)
    │   ├── 参考 UNet (UNet2DConditionModel)
    │   ├── 去噪 UNet (UNet3DConditionModel)
    │   ├── 运动编码器 (MotionEncoder)
    │   ├── 姿态编码器 (LivePortraitMotionExtractor)
    │   └── 姿态引导器 (PoseGuider)
    │
    ├──► 加载预训练权重
    │
    ├──► 创建 Pipeline
    │   ├── Pose2VideoPipeline (标准模式)
    │   └── Pose2VideoPipeline_Stream (流式模式)
    │
    └──► 处理循环
        ├── 对每个测试用例 (参考图 + 驱动视频):
        │   │
        │   ├──► 使用 MediaPipe 裁剪人脸
        │   │   ├── crop_face(reference_image)
        │   │   └── crop_face(driving_video_frames)
        │   │
        │   ├──► 准备张量
        │   │   ├── reference_tensor (参考图像张量)
        │   │   ├── face_tensor (驱动人脸张量)
        │   │   └── pose_tensor (姿态张量)
        │   │
        │   └──► 生成视频
        │       └── pipeline.__call__(...)
        │
        └──► 保存结果
```

**关键代码位置:**
- 入口: `inference_offline.py:main()`
- Pipeline 初始化: `inference_offline.py:~200-250`
- 生成循环: `inference_offline.py:~300-400`

### 2.2 在线推理入口 (`inference_online.py`)

**调用链:**

```
inference_online.py
    │
    ├──► FastAPI 应用启动
    │   ├── WebSocket 连接管理
    │   └── MJPEG 流式输出
    │
    ├──► 多进程架构
    │   ├── 主进程: Web 服务器
    │   └── 生成进程: 视频生成
    │
    └──── 队列通信
        ├── 输入队列: 接收驱动帧
        └── 输出队列: 发送生成结果
```

---

## 三、核心 Pipeline 调用链

### 3.1 Pose2VideoPipeline (标准模式)

**文件:** `src/pipelines/pipeline_pose2vid.py`

```
Pose2VideoPipeline.__call__()
    │
    ├──► 1. 编码参考图像
    │   ├── reference_image → VAE → ref_latents
    │   └── ref_latents → Reference UNet → ref_features
    │
    ├──► 2. 准备驱动信号
    │   ├── face_tensor → Motion Encoder → motion_features
    │   └── pose_tensor → Pose Encoder → pose_features
    │
    ├──► 3. 初始化噪声
    │   └── latents = randn(batch, channels, frames, height, width)
    │
    ├──► 4. 去噪循环 (DDIM)
    │   │
    │   └── for t in timesteps:
    │       │
    │       ├──► 4.1 UNet 前向传播
    │       │   ├── 输入: latents, t, ref_features, motion_features, pose_features
    │       │   └── 输出: noise_pred
    │       │
    │       ├──► 4.2 DDIM 更新
    │       │   └── latents = ddim_step(latents, noise_pred, t)
    │       │
    │       └──► 4.3 参考控制更新
    │           └── update_reference_attention(latents)
    │
    └──► 5. 解码
        └── latents → VAE decode → video_frames
```

### 3.2 Pose2VideoPipeline_Stream (流式模式)

**文件:** `src/pipelines/pipeline_pose2vid.py`

**流式策略核心:**

```
Pose2VideoPipeline_Stream.__call__()
    │
    ├──► 参数设置
    │   ├── window_size = 4    # 时序窗口大小
    │   ├── adaptive_step = 4  # 自适应步数
    │   └── history_keyframe  # 历史关键帧机制
    │
    └──► 滑动窗口生成
        │
        └── for chunk in video_chunks(window_size):
            │
            ├──► 1. 准备窗口数据
            │   ├── 当前窗口帧
            │   ├── 历史关键帧 (用于时序一致性)
            │   └── 运动和姿态特征
            │
            ├──► 2. 窗口内去噪
            │   └── [标准去噪流程] (adaptive_step 步)
            │
            ├──► 3. 保存历史
            │   └── 将当前窗口结果存为历史关键帧
            │
            └──► 4. 解码输出
                └── VAE decode → 输出帧
```

**VRAM 优化机制:**
- 时间窗口化: 一次只处理 4 帧
- 历史关键帧: 保持时序一致性
- 自适应步数: 减少推理步数

**关键代码位置:**
- Pipeline 定义: `src/pipelines/pipeline_pose2vid.py:Pose2VideoPipeline_Stream`
- 流式生成逻辑: `src/pipelines/pipeline_pose2vid.py:__call__` (Stream 版本)

---

## 四、模型组件调用链

### 4.1 参考 UNet (Reference UNet)

**文件:** `src/models/unet_2d_condition.py`

```
Reference UNet 前向传播
    │
    ├──► 输入: 参考图像 latents
    │
    ├──► CLIP 图像编码器
    │   └── reference_image → clip_embeddings
    │
    ├──► UNet 2D 编码
    │   ├── 下采样层
    │   ├── ResNet 块
    │   └── Attention 层
    │
    └──► 输出: 多尺度特征图
        ├── feat_8x (1/8 分辨率)
        ├── feat_16x (1/16 分辨率)
        ├── feat_32x (1/32 分辨率)
        └── key/value caches (用于交叉注意力)
```

**用途:**
- 提取参考图像的外观特征
- 为去噪 UNet 提供参考注意力
- 缓存 key-value 对以提高效率

### 4.2 去噪 UNet (Denoising UNet 3D)

**文件:** `src/models/unet_3d.py`

```
UNet3DConditionModel 前向传播
    │
    ├──► 输入
    │   ├── latents: 当前噪声 latents (B, C, T, H, W)
    │   ├── timestep: 去噪时间步
    │   ├── motion_features: 运动特征
    │   ├── pose_features: 姿态特征
    │   └── ref_features: 参考特征 (来自 Reference UNet)
    │
    ├──► 3D 卷积下采样
    │   └── Conv3D 层逐步降低分辨率
    │
    ├──► 时序运动模块 (Temporal Motion Module)
    │   │
    │   ├──► Temporal Attention
    │   │   ├── 帧间注意力机制
    │   │   └── 保持时序一致性
    │   │
    │   └──► Spatial Attention
    │       ├── 空间自注意力
    │       └── 参考交叉注意力
    │
    ├──► 中间层处理
    │   └── ResNet3D 块 + Attention
    │
    ├──► 3D 卷积上采样
    │   └── Conv3D 层逐步恢复分辨率
    │
    └──► 输出
        └── noise_pred: 预测的噪声 (B, C, T, H, W)
```

**关键技术:**
- 时序注意力: 跨帧信息传递
- 参考交叉注意力: 保持与参考图像的外观一致性
- 运动条件注入: 融合运动和姿态信号

### 4.3 运动编码器 (Motion Encoder)

**文件:** `src/models/motion_encoder/motion_encoder.py`

```
MotionEncoder
    │
    ├──► 输入: 驱动人脸图像 (face_tensor)
    │
    ├──► ConvNeXtV2 骨干网络
    │   ├── Patchify: 图像分块
    │   ├── ConvNeXt Blocks: 特征提取
    │   └── 层归一化
    │
    └──► 输出: 1D 面部运动嵌入 (motion_features)
```

**用途:**
- 提取精细面部表情变化
- 生成用于交叉注意力的运动条件

### 4.4 姿态编码器 (Pose Encoder)

**文件:** `src/liveportrait/motion_extractor.py`

```
LivePortraitMotionExtractor
    │
    ├──► 输入: 驱动图像
    │
    ├──► 提取 3D 关键点
    │   ├── kc: 规范化关键点 (21 个)
    │   ├── R: 旋转参数 (pitch, yaw, roll)
    │   ├── t: 平移参数
    │   └── s: 缩放参数
    │
    ├──► 关键点变换
    │   └── kd = s ⋅ kc ⋅ R + t
    │
    ├──► 姿态插值 (可选)
    │   └── 时序平滑处理
    │
    └──► 输出: 3D 姿态特征 (pose_features)
```

### 4.5 姿态引导器 (Pose Guider)

**文件:** `src/models/pose_guider.py`

```
PoseGuider
    │
    ├──► 输入: 3D 关键点 (kd)
    │
    ├──► 空间到通道变换
    │   └── 将 (H, W, 3) 关键点图展平为通道
    │
    ├──► 2D 卷积层
    │   ├── Conv2d 逐步增加通道数
    │   └── 生成引导特征图
    │
    └──► 输出: 姿态引导特征
```

**用途:**
- 将 3D 关键点转换为空间特征图
- 注入到去噪 UNet 的中间层

---

## 五、Wrapper 层调用链

### 5.1 PersonaLive 包装类

**文件:** `src/wrapper.py`

```
PersonaLive 类
    │
    ├──► 初始化
    │   ├── 加载所有模型组件
    │   ├── 设置推理参数
    │   └── 初始化参考图像缓存
    │
    ├──► 参考图像融合
    │   ├── fuse_reference(image)
    │   ├── 编码参考图像
    │   └── 提取参考特征
    │
    ├──► 流式生成
    │   ├── generate_stream(driving_frames)
    │   ├── 维护时序状态
    │   └── 管理关键帧队列
    │
    └──── 关键帧机制
        ├── 定期更新关键帧
        ├── 保持时序一致性
        └── 防止长期漂移
```

**关键方法:**
- `fuse_reference()`: 融合新参考图像
- `generate_stream()`: 流式生成接口
- `update_keyframe()`: 更新历史关键帧

---

## 六、完整视频生成流程

### 6.1 输入处理阶段

```
输入处理
    │
    ├──► 参考图像 (Reference Image)
    │   │
    │   ├──► MediaPipe 人脸检测
    │   │   └── detect_face_crop()
    │   │
    │   ├──► 人脸对齐和裁剪
    │   │   └── crop_face()
    │   │
    │   └──► 编码为张量
    │       └── (1, 3, 256, 256)
    │
    └──► 驱动视频 (Driving Video)
        │
        ├──► 逐帧处理
        │
        ├──► MediaPipe 人脸裁剪
        │   └── crop_face(frame)
        │
        ├──► 运动提取
        │   └── MotionEncoder(face_crop) → motion_features
        │
        └──── 姿态提取
            └── LivePortraitMotionExtractor(frame) → pose_features
```

### 6.2 生成阶段 (标准模式)

```
视频生成 (标准模式)
    │
    ├──► 1. 初始化
    │   ├── 创建噪声 latents: (B, C, T, H/8, W/8)
    │   ├── 准备运动特征
    │   ├── 准备姿态特征
    │   └── 加载参考特征
    │
    ├──► 2. 去噪循环 (DDIM, 50 步)
    │   │
    │   └── for t in [T, ..., 1]:
    │       │
    │       ├──► UNet 前向传播
    │       │   ├── 输入: latents, t, motion_features, pose_features
    │       │   ├── 参考交叉注意力
    │       │   ├── 时序注意力
    │       │   └── 输出: noise_pred
    │       │
    │       ├──► DDIM 更新
    │       │   ├── x_0 = (latents - sqrt(1-α̅_t) * noise_pred) / sqrt(α̅_t)
    │       │   ├── latents_next = sqrt(α̅_{t-1}) * x_0 + sqrt(1-α̅_{t-1}) * noise_pred
    │       │   └── latents = latents_next
    │       │
    │       └──── 参考控制更新
    │           └── 更新交叉注意力缓存
    │
    └──► 3. 解码
        ├── latents → VAE Decoder
        └── 输出: (B, T, 3, H, W) 视频帧
```

### 6.3 生成阶段 (流式模式)

```
视频生成 (流式模式)
    │
    ├──► 参数
    │   ├── window_size = 4
    │   ├── adaptive_step = 4
    │   └── stride = 4
    │
    └──► 滑动窗口处理
        │
        └── for i in range(0, total_frames, stride):
            │
            ├──► 1. 获取当前窗口
            │   ├── window_frames = frames[i:i+window_size]
            │   └── window_motion = motion[i:i+window_size]
            │
            ├──► 2. 历史关键帧机制
            │   ├── 加载历史关键帧 (前一个窗口的结果)
            │   ├── 拼接到当前窗口
            │   └── 保持时序连续性
            │
            ├──► 3. 窗口内去噪
            │   │
            │   └── for t in timesteps:  (仅 4 步)
            │       │
            │       ├──► UNet 前向传播
            │       │   ├── 当前窗口 latents
            │       │   ├── 历史关键帧 latents
            │       │   ├── 运动和姿态条件
            │       │   └── 参考特征
            │       │
            │       └──── 噪声预测和更新
            │
            ├──► 4. 保存关键帧
            │   └── 将当前窗口结果存为历史
            │
            └──── 5. 解码输出
                └── VAE decode → 输出窗口帧
```

**VRAM 使用优化:**
- 一次只处理 4 帧
- 历史关键帧重用
- 自适应减少步数 (4 步 vs 50 步)

---

## 七、关键代码位置索引

| 功能 | 文件 | 位置 |
|------|------|------|
| 离线推理入口 | `inference_offline.py` | `main()` 函数 |
| 在线推理入口 | `inference_online.py` | `main()` 函数 |
| 标准 Pipeline | `src/pipelines/pipeline_pose2vid.py` | `Pose2VideoPipeline` 类 |
| 流式 Pipeline | `src/pipelines/pipeline_pose2vid.py` | `Pose2VideoPipeline_Stream` 类 |
| 参考 UNet | `src/models/unet_2d_condition.py` | `UNet2DConditionModel` 类 |
| 去噪 UNet | `src/models/unet_3d.py` | `UNet3DConditionModel` 类 |
| 运动编码器 | `src/models/motion_encoder/motion_encoder.py` | `MotionEncoder` 类 |
| 姿态提取器 | `src/liveportrait/motion_extractor.py` | `LivePortraitMotionExtractor` 类 |
| 姿态引导器 | `src/models/pose_guider.py` | `PoseGuider` 类 |
| Wrapper | `src/wrapper.py` | `PersonaLive` 类 |
| DDIM 调度器 | `src/scheduler/ddim.py` | `DDIMScheduler` 类 |

---

## 八、数据流图

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                          PersonaLive 数据流与调用链                                    │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  输入阶段                                                                            │
│  ┌──────────────┐         ┌──────────────┐                                         │
│  │ 参考图像      │         │ 驱动视频      │                                         │
│  │ (Reference)  │         │ (Driving)    │                                         │
│  └──────┬───────┘         └──────┬───────┘                                         │
│         │                        │                                                  │
│         ▼                        ▼                                                  │
│  ┌──────────────┐         ┌──────────────────┐                                      │
│  │ MediaPipe    │         │ 逐帧处理          │                                      │
│  │ 人脸裁剪     │         │ Frame Processing  │                                      │
│  └──────┬───────┘         └──────┬───────────┘                                      │
│         │                        │                                                  │
│         │                ┌───────┴────────┐                                         │
│         │                ▼                ▼                                         │
│         │         ┌─────────────┐  ┌─────────────┐                                  │
│         │         │Motion       │  │Pose         │                                  │
│         │         │Encoder      │  │Extractor    │                                  │
│         │         └──────┬──────┘  └──────┬──────┘                                  │
│         │                │                │                                         │
│         │                │                │                                         │
│  编码阶段                ▼                ▼                                         │
│  ┌──────────────┐   motion_features  pose_features                                 │
│  │ CLIP Image   │   (1D 表情嵌入)    (3D 关键点)                                    │
│  │ Encoder      │        │                │                                         │
│  └──────┬───────┘        │                │                                         │
│         │                │                │                                         │
│         ▼                │                │                                         │
│  ┌──────────────┐        │                │                                         │
│  │ Reference    │        │                │                                         │
│  │ UNet (2D)    │        │                │                                         │
│  └──────┬───────┘        │                │                                         │
│         │                │                │                                         │
│         ▼                │                │                                         │
│  ref_features            │                │                                         │
│  (多尺度特征 + K/V Cache) │                │                                         │
│         │                │                │                                         │
│         └────────────────┴────────────────┘                                         │
│                            │                                                         │
│                            ▼                                                         │
│  ┌─────────────────────────────────────────────┐                                    │
│  │         生成阶段                              │                                    │
│  │                                             │                                    │
│  │  ┌─────────────────────────────────────┐   │                                    │
│  │  │      初始化噪声 latents             │   │                                    │
│  │  │  latents ~ N(0, I)                 │   │                                    │
│  │  └──────────────┬──────────────────────┘   │                                    │
│  │                 │                           │                                    │
│  │                 ▼                           │                                    │
│  │  ┌─────────────────────────────────────┐   │                                    │
│  │  │       去噪循环 (DDIM)               │   │                                    │
│  │  │                                     │   │                                    │
│  │  │  for t in timesteps:               │   │                                    │
│  │  │      │                             │   │                                    │
│  │  │      ├──► UNet3D 前向传播          │   │                                    │
│  │  │      │    ├── Temporal Attention   │   │                                    │
│  │  │      │    ├── Spatial Attention    │   │                                    │
│  │  │      │    ├── Cross Attn (ref)     │   │                                    │
│  │  │      │    └── Motion/Pose Cond     │   │                                    │
│  │  │      │                             │   │                                    │
│  │  │      ├──► 预测噪声                  │   │                                    │
│  │  │      │    noise_pred = UNet(...)   │   │                                    │
│  │  │      │                             │   │                                    │
│  │  │      └──► DDIM 更新                 │   │                                    │
│  │  │          latents = ddim_step(...)  │   │                                    │
│  │  └──────────────┬──────────────────────┘   │                                    │
│  └─────────────────┼───────────────────────────┘                                    │
│                    │                                                               │
│                    ▼                                                               │
│  ┌─────────────────────────────────────────────┐                                    │
│  │         解码阶段                              │                                    │
│  │                                             │                                    │
│  │  latents ──► VAE Decoder ──► video_frames   │                                    │
│  │                                             │                                    │
│  └──────────────┬──────────────────────────────┘                                    │
│                 │                                                                 │
│                 ▼                                                                 │
│  ┌─────────────────────────────────────────────┐                                    │
│  │         输出                                  │                                    │
│  │                                             │                                    │
│  │  视频帧: (B, T, 3, H, W)                    │                                    │
│  │  格式: [0, 1] 范围, RGB                     │                                    │
│  │                                             │                                    │
│  └─────────────────────────────────────────────┘                                    │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

---

## 九、流式模式详细调用链

```
流式生成 (Streaming Generation)
    │
    ├──► 启动流式会话
    │   ├── 加载参考图像并编码
    │   ├── 初始化历史关键帧队列
    │   └── 设置流式参数
    │
    └──► 逐窗口处理循环
        │
        └── for window_idx in range(num_windows):
            │
            ├──► 1. 准备窗口数据
            │   ├── current_frames = frames[window_idx:window_idx+window_size]
            │   ├── current_motion = motion[window_idx:window_idx+window_size]
            │   ├── current_pose = pose[window_idx:window_idx+window_size]
            │   └── history_keyframe = keyframe_queue[-1]  # 前一个窗口的结果
            │
            ├──► 2. 初始化窗口 latents
            │   ├── window_latents = randn(window_size)
            │   └── concat(window_latents, history_keyframe)
            │
            ├──► 3. 快速去噪 (adaptive_step = 4)
            │   │
            │   └── for t in timesteps:  # 仅 4 步
            │       │
            │       ├──► UNet 前向传播
            │       │   ├── 输入: [window_latents, history_keyframe]
            │       │   ├── 条件: [current_motion, current_pose]
            │       │   └── 参考: ref_features
            │       │
            │       └──── 噪声预测和更新
            │
            ├──► 4. 提取窗口结果
            │   ├── window_output = latents[:window_size]
            │   └── 分离当前窗口和历史关键帧
            │
            ├──► 5. 更新关键帧队列
            │   ├── keyframe_queue.append(window_output)
            │   └── 保持队列长度限制
            │
            └──── 6. 解码输出
                └── VAE decode(window_output) → output_frames
```

**关键优化:**
- **时间局部性**: 每次只处理 4 帧
- **历史复用**: 重用前一个窗口的结果
- **快速推理**: 仅 4 步去噪
- **VRAM 友好**: 可在 12GB VRAM 上运行

---

## 十、配置文件系统

### 10.1 配置文件位置

```
configs/
├── inference/
│   └── personalive_inference.yaml    # 模型推理配置
└── prompts/
    ├── personalive_offline.yaml      # 离线模式配置
    └── personalive_online.yaml       # 在线模式配置
```

### 10.2 关键配置参数

```yaml
# 模型路径
pretrained_models:
  vae: "path/to/vae"
  unet: "path/to/unet"
  motion_encoder: "path/to/motion_encoder"

# 推理参数
inference:
  num_steps: 50           # 标准模式去噪步数
  adaptive_step: 4        # 流式模式去噪步数
  guidance_scale: 1.0     # 引导系数
  window_size: 4          # 流式窗口大小

# 硬件加速
acceleration:
  use_xformers: true      # 使用 XFormers
  use_tensorrt: false     # 使用 TensorRT
```

---

## 十一、总结

PersonaLive 的架构设计具有以下特点:

1. **模块化设计**: 各组件职责清晰，易于维护
2. **流式优化**: 支持长视频生成，VRAM 友好
3. **双模式支持**: 离线批量 + 在线流式
4. **条件控制**: 运动和姿态双条件输入
5. **参考控制**: 通过参考注意力保持外观一致性

核心创新点:
- 滑动窗口流式生成策略
- 历史关键帧机制
- 运动-姿态双条件融合
- 参考交叉注意力控制
