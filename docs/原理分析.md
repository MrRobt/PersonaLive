---
  PersonaLive 实现原理分析

  一、核心架构概述

  ┌─────────────────────────────────────────────────────────────────────────────┐
  │                        PersonaLive 三阶段训练流程                             │
  ├─────────────────────────────────────────────────────────────────────────────┤
  │                                                                              │
  │  ┌─────────────────────┐    ┌──────────────────────┐    ┌─────────────────┐ │
  │  │  阶段1: 混合运动控制  │ →  │  阶段2: 外观蒸馏     │ →  │ 阶段3: 流式生成  │ │
  │  │  Hybrid Motion      │    │  Appearance          │    │  Streaming      │ │
  │  │  Training           │    │  Distillation        │    │  Generation     │ │
  │  └─────────────────────┘    └──────────────────────┘    └─────────────────┘ │
  │                                                                              │
  │  目标: 表情丰富的       目标: 消除冗余外观细化       目标: 实时、稳定的      │
  │        图像级运动控制          提高推理效率              长视频生成          │
  │                                                                              │
  └─────────────────────────────────────────────────────────────────────────────┘

  ---
  二、关于骨骼动作驱动 — 核心问题解答

  ✅ 是的，PersonaLive 使用骨骼/关键点数据驱动视频生成！

  但它使用的是 3D 隐式关键点 (3D Implicit Keypoints)，而非传统的2D骨架：

  | 驱动信号类型  | 说明           | 用途                     |
  |---------------|----------------|--------------------------|
  | 3D 隐式关键点 | 21个面部关键点 | 控制头部姿态、位置、缩放 |
  | 隐式面部表征  | 1D面部运动嵌入 | 控制精细面部表情         |

  ---
  三、3D关键点提取原理

  根据论文公式(2-3)：

  从驱动图像 ID 提取 3D 参数：
  kc,d, Rd, td, sd = Ek(ID)

  从源图像 IR 提取 3D 参数：
  kc,s, Rs, ts, ss = Ek(IR)

  其中:
  - kc: 规范化关键点 (21个)
  - R: 旋转参数 (pitch, yaw, roll)
  - t: 平移参数
  - s: 缩放参数

  关键点变换公式：
  kd = sd ⋅ kc,s ⋅ Rd + td

  这些变换后的关键点被映射到像素空间，并通过 PoseGuider 注入去噪骨干网络。

  ---
  四、完整的运动控制流程

  ┌──────────────────────────────────────────────────────────────────────────┐
  │                         PersonaLive 运动控制流程                            │
  ├──────────────────────────────────────────────────────────────────────────┤
  │                                                                           │
  │  驱动图像 (ID)                                                           │
  │       │                                                                   │
  │       ├──► [Face Motion Extractor] ──► mf (1D面部运动嵌入)                │
  │  (精细表情控制)          │                                                  │
  │                        └──► 注入 Cross-Attention 层                       │
  │                                                                           │
  │       ├──► [3D Keypoint Extractor] ──► kd, Rd, td, sd                    │
  │  (头部姿态控制)               │                                            │
  │                             ├──► kd = sd⋅kc,s⋅Rd+td (变换)              │
  │                             └──► 注入 PoseGuider                         │
  │                                                                           │
  │  参考图像 (IR) ──► [Reference Net] ──► 外观特征                            │
  │                                                                           │
  │  ┌─────────────────────────────────────────────────────────────────┐     │
  │  │               UNet 3D 去噪器 (时序扩散模型)                       │     │
  │  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐               │     │
  │  │  │ Cross-Attn  │  │ PoseGuider  │  │ Temporal    │               │     │
  │  │  │ + mf        │  │ + kd        │  │ Module      │               │     │
  │  │  └─────────────┘  └─────────────┘  └─────────────┘               │     │
  │  └─────────────────────────────────────────────────────────────────┘     │
  │                              │                                            │
  │                              ▼                                            │
  │                         生成视频帧                                          │
  │                                                                           │
  └──────────────────────────────────────────────────────────────────────────┘

  ---
  五、与你的项目集成建议

  基于PersonaLive的技术方案，如果你想在你的 control 项目中实现类似功能：

  1. 骨骼动作数据格式

  # 你可以定义如下的骨骼数据结构
  from dataclasses import dataclass
  from typing import List, Tuple
  import numpy as np

  @dataclass
  class PoseKeypoints3D:
      """3D姿态关键点数据结构"""
      keypoints: np.ndarray  # (21, 3) 21个3D关键点
      rotation: Tuple[float, float, float]  # (pitch, yaw, roll)
      translation: np.ndarray  # (3,) 平移向量
      scale: float  # 缩放因子

  2. 服务层设计

  参考你的项目架构，可以在 src/api/services/ 下创建：

  # src/api/services/personalive_service.py
  class PersonaLiveService:
      """PersonaLive 数字人视频生成服务"""

      def __init__(self):
          self.motion_extractor = self._load_motion_extractor()
          self.pipeline = self._load_pipeline()

      def generate_animation(
          self,
          reference_image: str,
          driving_video: str,
          pose_data: Optional[PoseKeypoints3D] = None
      ) -> str:
          """
          生成数字人动画

          Args:
              reference_image: 参考图像路径
              driving_video: 驱动视频路径
              pose_data: 可选的3D姿态数据（如果使用骨骼驱动）

          Returns:
              生成视频的保存路径
          """
          pass

  3. API控制器

  # src/api/v2/controllers/digital_human_controller_v2.py
  class DigitalHumanControllerV2(BaseController):
      """数字人生成控制器v2"""

      def __init__(self, personalive_service: PersonaLiveService):
          super().__init__()
          self.personalive_service = personalive_service

  ---
  六、关键技术对比

  | 技术     | PersonaLive             | 传统方法              | 优势             |
  |----------|-------------------------|-----------------------|------------------|
  | 驱动信号 | 3D隐式关键点 + 面部嵌入 | 2D关键点 / 原始视频帧 | 更灵活的姿态控制 |
  | 推理步数 | 4步 (蒸馏后)            | 20+步                 | 7-22倍加速       |
  | 生成模式 | 微块流式生成            | 固定分块生成          | 实时、低延迟     |
  | 训练策略 | 滑动训练 + 历史关键帧   | 标准训练              | 稳定长期生成     |

  ---
  七、限制与注意事项

  ⚠️ 重要限制：

  1. 仅支持肖像动画：PersonaLive专门针对人脸和头部设计，不支持全身骨骼动画
  2. 21个面部关键点：使用的是LivePortrait的面部关键点，不是全身骨架
  3. 训练域限制：主要在人类面部数据上训练，对卡通/动物等非人类肖像泛化性有限

  ---
  八、总结

  ✅ 可以使用骨骼动作驱动，但是...

  - 支持：3D面部关键点驱动（头部姿态 + 面部表情）
  - 不支持：全身骨骼动画（如手臂、腿部动作）

  💡 如果你的需求是全身数字人：

  1. 可以将PersonaLive用于头部/面部部分
  2. 结合其他全身动画方案（如如AnimateAnyone、MagicPose等）处理身体部分
  3. 通过视频合成/融合技术组合最终结果

  ---
  Sources:
  - https://arxiv.org/html/2512.11253v1
  - https://github.com/MrRobt/PersonaLive