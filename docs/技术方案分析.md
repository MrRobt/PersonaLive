---
  整体流程概览

  ┌─────────────────────────────────────────────────────────────────────────────┐
  │                  关键点 → 像素 生成流程                                        │
  ├─────────────────────────────────────────────────────────────────────────────┤
  │                                                                              │
  │  输入: 21个3D关键点 + 6D姿态                                                  │
  │         │                                                                    │
  │         ▼                                                                    │
  │  ┌─────────────────────────────────────────────────────────────────────┐    │
  │  │                    步骤1: 姿态特征提取                               │    │
  │  │                    PoseGuider: 关键点 → 空间特征图                   │    │
  │  └────────────────────────────────────┬────────────────────────────────┘    │
  │                                       │                                      │
  │                                       ▼                                      │
  │  ┌─────────────────────────────────────────────────────────────────────┐    │
  │  │                    步骤2: 潜空间初始化                               │    │
  │  │                    随机噪声 latents (B, C, T, H, W)                  │    │
  │  └────────────────────────────────────┬────────────────────────────────┘    │
  │                                       │                                      │
  │                                       ▼                                      │
  │  ┌─────────────────────────────────────────────────────────────────────┐    │
  │  │                    步骤3: 扩散去噪循环                               │    │
  │  │                    UNet 3D + 条件注入                                │    │
  │  └────────────────────────────────────┬────────────────────────────────┘    │
  │                                       │                                      │
  │                                       ▼                                      │
  │  ┌─────────────────────────────────────────────────────────────────────┐    │
  │  │                    步骤4: VAE 解码                                   │    │
  │  │                    Latents → RGB 像素                                │    │
  │  └────────────────────────────────────┬────────────────────────────────┘    │
  │                                       │                                      │
  │                                       ▼                                      │
  │  输出: 视频帧 (B, T, 3, H, W)                                                   │
  │                                                                              │
  └─────────────────────────────────────────────────────────────────────────────┘

  ---
  一、姿态特征提取

  1.1 PoseGuider 架构

  PoseGuider 将稀疏的 3D 关键点转换为密集的空间特征图：

  class PoseGuider(nn.Module):
      """关键点 → 空间特征图"""

      def __init__(self):
          super().__init__()
          # 将 3D 关键点投影到 2D 空间
          self.projection = nn.Conv2d(3, 64, kernel_size=7, padding=3)

          # 逐步增加通道数
          self.layers = nn.Sequential(
              nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 1/2
              nn.SiLU(),
              nn.Conv2d(128, 256, 4, stride=2, padding=1), # 1/4
              nn.SiLU(),
              nn.Conv2d(256, 512, 4, stride=2, padding=1), # 1/8
              nn.SiLU(),
          )

      def forward(self, pose_map):
          """
          输入: pose_map (B, 3, H, W) - 关键点热图
          输出: (B, 512, H/8, W/8) - 姿态特征
          """
          x = self.projection(pose_map)
          x = self.layers(x)
          return x

  1.2 关键点投影

  3D 关键点 (21, 3)
          │
          ├──► 透视投影
          │     x_2d = x_3d / z_3d
          │     y_2d = y_3d / z_3d
          │
          ▼
  2D 热图 (H, W)
          │
          ├──► 高斯扩散 (使关键点周围有响应)
          │
          ▼
  稠密特征图 (3, H, W)

  ---
  二、UNet 3D 去噪架构

  2.1 整体结构

  UNet 3D Condition Model
  ├── Encoder (下采样)
  │   ├── Conv3D × N
  │   ├── Temporal Motion Module
  │   └── Downsample
  ├── Middle (处理)
  │   └── ResNet3D Blocks
  └── Decoder (上采样)
      ├── Upsample
      ├── Temporal Motion Module
      └── Conv3D × N

  2.2 详细代码结构

  class UNet3DConditionModel(nn.Module):
      """3D 条件去噪 UNet"""

      def __init__(self):
          super().__init__()

          # 时间步嵌入
          self.time_embedding = TimestepEmbedding()

          # 下采样编码器
          self.down_blocks = nn.ModuleList([
              # Block 0: 1x 分辨率
              CrossAttnDownBlock3D(
                  in_channels=4,      # latents
                  out_channels=320,
                  num_layers=2,
                  temb_channels=1280,
                  transformer_dim_per_head=64,
              ),
              # Block 1: 1/2 分辨率
              CrossAttnDownBlock3D(320, 640, 2, ...),
              # Block 2: 1/4 分辨率
              CrossAttnDownBlock3D(640, 1280, 2, ...),
              # Block 3: 1/8 分辨率
              DownBlock3D(1280, 1280),
          ])

          # 中间层
          self.mid_block = UNetMidBlock3DCrossAttn(1280, ...)

          # 上采样解码器
          self.up_blocks = nn.ModuleList([
              # Block 0: 1/8 → 1/4
              UpBlock3D(1280, 1280),
              # Block 1: 1/4 → 1/2
              CrossAttnUpBlock3D(2560, 640, 2, ...),  # skip connection
              # Block 2: 1/2 → 1
              CrossAttnUpBlock3D(1280, 320, 2, ...),
              # Block 3: 输出
              UpBlock3D(640, 320),
          ])

          # 输出卷积
          self.conv_out = nn.Conv2d(320, 4, 3, padding=1)

      def forward(
          self,
          sample,           # (B, C, T, H, W) 噪声 latents
          timestep,
          encoder_hidden_states,  # (B, L, D) 参考特征
          motion_features,        # (B, T, D) 运动嵌入
          pose_features,          # (B, 512, H, W) 姿态特征
      ):
          # 1. 时间步嵌入
          emb = self.time_embedding(timestep)

          # 2. 编码路径
          down_block_res_samples = ()
          for down_block in self.down_blocks:
              sample, res_samples = down_block(
                  sample,
                  emb,
                  encoder_hidden_states=encoder_hidden_states,
                  motion_features=motion_features,
                  pose_features=pose_features,
              )
              down_block_res_samples += res_samples

          # 3. 中间处理
          sample = self.mid_block(
              sample,
              emb,
              encoder_hidden_states=encoder_hidden_states,
              motion_features=motion_features,
          )

          # 4. 解码路径 (带 skip connection)
          for i, up_block in enumerate(self.up_blocks):
              res_samples = down_block_res_samples[-(i+1):]
              sample = up_block(
                  sample,
                  emb,
                  res_hidden_states=res_samples,
                  encoder_hidden_states=encoder_hidden_states,
                  motion_features=motion_features,
              )

          # 5. 输出
          sample = self.conv_out(sample)
          return sample

  ---
  三、时序运动模块

  3.1 核心机制

  这是 PersonaLive 实现表情迁移的关键：

  class TemporalMotionModule(nn.Module):
      """时序运动模块"""

      def __init__(self, channels):
          super().__init__()

          # 空间自注意力
          self.spatial_attn = SpatialAttention(channels)

          # 时序注意力
          self.temporal_attn = TemporalAttention(channels)

          # 交叉注意力 (参考控制)
          self.cross_attn = CrossAttention(
              query_dim=channels,
              key_dim=channels,
              value_dim=channels,
              num_heads=8,
          )

          # 运动条件门控
          self.motion_gate = MotionGating(channels)

      def forward(
          self,
          hidden_states,      # (B, C, T, H, W)
          encoder_hidden_states,  # (B, L, D) 参考特征
          motion_features,        # (B, T, D) 运动嵌入
      ):
          batch_size, channels, frames, height, width = hidden_states.shape

          # Reshape: (B, C, T, H, W) → (B*T, C, H, W)
          hidden_states = rearrange(hidden_states, "b c t h w -> (b t) c h w")

          # 1. 空间自注意力
          hidden_states = self.spatial_attn(hidden_states)

          # 2. 交叉注意力 (参考控制)
          hidden_states = self.cross_attn(
              hidden_states,
              encoder_hidden_states,
          )

          # 3. 运动条件融合
          hidden_states = self.motion_gate(hidden_states, motion_features)

          # 4. 时序注意力
          hidden_states = rearrange(hidden_states, "(b t) c h w -> b c t h w", b=batch_size)
          hidden_states = self.temporal_attn(hidden_states)

          return hidden_states

  3.2 各注意力机制详解

  空间自注意力

  class SpatialAttention(nn.Module):
      """空间自注意力 - 保持单帧内特征一致性"""

      def forward(self, x):  # (B*T, C, H, W)
          # Reshape to sequence of patches
          b, c, h, w = x.shape
          x = rearrange(x, "b c h w -> b (h w) c")

          # Multi-head self-attention
          q = self.to_q(x)
          k = self.to_k(x)
          v = self.to_v(x)

          attn = torch.matmul(q, k.transpose(-2, -1)) * scale
          attn = softmax(attn)
          out = torch.matmul(attn, v)

          return rearrange(out, "b (h w) c -> b c h w", h=h, w=w)

  时序注意力

  class TemporalAttention(nn.Module):
      """时序注意力 - 帧间信息传递"""

      def forward(self, x):  # (B, C, T, H, W)
          b, c, t, h, w = x.shape
          x = rearrange(x, "b c t h w -> (b h w) t c")

          # 时序注意力
          attn = self.multihead_attn(x, x, x)

          return rearrange(attn, "(b h w) t c -> b c t h w", b=b, h=h, w=w)

  交叉注意力 (参考控制)

  class CrossAttention(nn.Module):
      """交叉注意力 - 与参考图像对齐"""

      def forward(self, x, context):  # x: 生成, context: 参考
          # x: (B*T, H*W, C_gen)
          # context: (B, L, C_ref)

          q = self.to_q(x)       # 来自生成帧
          k = self.to_k(context) # 来自参考图像
          v = self.to_v(context)

          # 注意力计算
          attn = torch.matmul(q, k.transpose(-2, -1)) * scale
          attn = softmax(attn)
          out = torch.matmul(attn, v)

          return out

  ---
  四、条件注入机制

  4.1 运动条件注入

  class MotionGating(nn.Module):
      """运动特征门控注入"""

      def forward(self, x, motion_features):
          """
          x: (B*T, C, H, W) 空间特征
          motion_features: (B, T, D) 运动嵌入
          """
          b, t, d = motion_features.shape

          # 扩展运动特征到空间维度
          motion = rearrange(motion_features, "b t d -> (b t) d 1 1")
          motion = F.interpolate(motion, size=x.shape[2:], mode='bilinear')

          # 门控机制
          gate = torch.sigmoid(self.gate_conv(torch.cat([x, motion], dim=1)))
          x = x * gate + motion * (1 - gate)

          return x

  4.2 姿态条件注入

  # PoseGuider 输出直接加到 UNet 中间层
  def forward(self, x, pose_features):
      # x: (B, C, T, H/8, W/8)
      # pose_features: (B, 512, H/8, W/8)

      # 广播到时序维度
      pose = pose_features.unsqueeze(2)  # (B, 512, 1, H/8, W/8)
      pose = pose.expand(-1, -1, x.shape[2], -1, -1)

      # 特征融合
      x = x + pose  # 残差连接

      return x

  ---
  五、扩散去噪过程

  5.1 DDIM 采样

  def ddim_step(
      model_output,     # 预测的噪声
      sample,           # 当前噪声 latents
      timestep,         # 当前时间步 t
      next_timestep,    # 下一步 t-1
      eta=0.0,          # 随机性参数 (0=确定性)
  ):
      """
      DDIM 更新公式:
      x_{t-1} = √(α̅_{t-1})·x_0 + √(1-α̅_{t-1}-σ_t²)·ε_t
      """
      # 1. 预测原始图像 x_0
      alpha_t = alpha_bars[timestep]
      pred_x0 = (sample - √(1-α_t) * model_output) / √(α_t)

      # 2. 计算方向指向 x_t
      alpha_t_prev = alpha_bars[next_timestep]
      dir_xt = √(1-α_t_prev) * model_output

      # 3. 计算 x_{t-1}
      x_prev = √(alpha_t_prev) * pred_x0 + dir_xt

      return x_prev

  5.2 完整去噪循环

  def denoise_loop(
      model,
      pose_features,      # 姿态条件
      motion_features,    # 运动条件
      ref_features,       # 参考特征
      num_steps=4,        # PersonaLive 仅用4步
  ):
      # 1. 初始化噪声
      latents = torch.randn(B, C, T, H, W)

      # 2. 去噪循环
      timesteps = [999, 749, 499, 249, 0]  # 4步采样

      for i, t in enumerate(timesteps[:-1]):
          t_next = timesteps[i+1]

          # 预测噪声
          noise_pred = model(
              sample=latents,
              timestep=t,
              encoder_hidden_states=ref_features,
              motion_features=motion_features,
              pose_features=pose_features,
          )

          # DDIM 更新
          latents = ddim_step(noise_pred, latents, t, t_next)

      return latents

  ---
  六、VAE 解码

  6.1 VAE 解码器

  class Decoder(nn.Module):
      """VAE 解码器: Latents → 像素"""

      def __init__(self):
          super().__init__()
          # 上采样 Conv
          self.conv_in = nn.Conv2d(4, 512, 3, padding=1)

          self.up_blocks = nn.ModuleList([
              # 1/8 → 1/4
              ResNetBlock(512, 512),
              Upsample(512),

              # 1/4 → 1/2
              ResNetBlock(512, 512),
              ResNetBlock(512, 512),
              Upsample(512),

              # 1/2 → 1
              ResNetBlock(512, 256),
              ResNetBlock(256, 256),
              Upsample(256),

              # 1 → 输出
              ResNetBlock(256, 128),
              ResNetBlock(128, 128),
          ])

          self.conv_out = nn.Conv2d(128, 3, 3, padding=1)

      def forward(self, z):
          """
          输入: (B, 4, H/8, W/8) latents
          输出: (B, 3, H, W) RGB 图像
          """
          x = self.conv_in(z)

          for block in self.up_blocks:
              x = block(x)

          x = self.conv_out(x)

          # 归一化到 [0, 1]
          x = (x / 2 + 0.5).clamp(0, 1)

          return x

  6.2 分块解码 (显存优化)

  def decode_chunked(vae_decoder, latents, chunk_size=4):
      """
      分块解码以节省显存

      latents: (B, C, T, H, W)
      """
      B, C, T, H, W = latents.shape

      outputs = []
      for i in range(0, T, chunk_size):
          chunk = latents[:, :, i:i+chunk_size]

          # 逐帧解码
          decoded = []
          for t in range(chunk.shape[2]):
              frame = vae_decoder(chunk[:, :, t])
              decoded.append(frame)

          outputs.extend(decoded)

      # 合并
      video = torch.stack(outputs, dim=1)  # (B, T, 3, H, W)
      return video

  ---
  七、完整生成流程总结

  ┌─────────────────────────────────────────────────────────────────────────────┐
  │                  完整的 关键点→像素 生成链路                                  │
  ├─────────────────────────────────────────────────────────────────────────────┤
  │                                                                              │
  │  输入: 3D关键点 + 6D姿态                                                      │
  │   │                                                                          │
  │   ▼                                                                          │
  │  ┌────────────────────────────────────────────────────────────────────┐     │
  │  │ PoseGuider: 关键点 → 姿态特征图 (512通道)                           │     │
  │  └────────────────────────────────────┬───────────────────────────────┘     │
  │                                       │                                      │
  │                                       ▼                                      │
  │  ┌────────────────────────────────────────────────────────────────────┐     │
  │  │ 初始化: 随机噪声 latents ~ N(0, I)                                 │     │
  │  └────────────────────────────────────┬───────────────────────────────┘     │
  │                                       │                                      │
  │                    ┌──────────────────┴──────────────────┐                │
  │                    ▼                                     ▼                │
  │  ┌─────────────────────┐              ┌─────────────────────┐              │
  │  │  参考特征           │              │  运动嵌入           │              │
  │  │  (Reference UNet)   │              │  (Motion Encoder)  │              │
  │  └─────────┬───────────┘              └─────────┬───────────┘              │
  │            │                                    │                          │
  │            └────────────┬───────────────────────┘                          │
  │                         ▼                                                 │
  │  ┌────────────────────────────────────────────────────────────────────┐    │
  │  │              UNet 3D 去噪循环 (4步 DDIM)                             │    │
  │  │                                                                     │    │
  │  │  每一步:                                                             │    │
  │  │  1. 空间自注意力 (帧内一致性)                                        │    │
  │  │  2. 交叉注意力 (参考对齐)                                           │    │
  │  │  3. 运动门控 (表情迁移)                                             │    │
  │  │  4. 时序注意力 (帧间平滑)                                           │    │
  │  │  5. 姿态特征注入 (头部控制)                                          │    │
  │  │  6. 残差连接                                                        │    │
  │  └────────────────────────────────────┬───────────────────────────────┘    │
  │                                       │                                      │
  │                                       ▼                                      │
  │  ┌────────────────────────────────────────────────────────────────────┐    │
  │  │ VAE Decoder: 去噪 latents → RGB 像素                               │    │
  │  └────────────────────────────────────┬───────────────────────────────┘    │
  │                                       │                                      │
  │                                       ▼                                      │
  │  输出: 视频帧 (B, T, 3, H, W) ∈ [0, 1]                                          │
  │                                                                              │
  └─────────────────────────────────────────────────────────────────────────────┘

  ---
  八、关键技术总结

  | 技术组件     | 作用        | 实现                   |
  |--------------|-------------|------------------------|
  | PoseGuider   | 关键点→特征 | 2D卷积 + 上采样        |
  | 空间自注意力 | 帧内一致性  | Transformer Self-Attn  |
  | 交叉注意力   | 外观保持    | Transformer Cross-Attn |
  | 运动门控     | 表情迁移    | FiLM + 门控机制        |
  | 时序注意力   | 帧间平滑    | Temporal Self-Attn     |
  | DDIM采样     | 快速去噪    | 4步确定性采样          |
  | VAE解码      | 潜空间→像素 | ResNet + 上采样        |

  这就是 PersonaLive 从关键点到像素的完整技术方案！